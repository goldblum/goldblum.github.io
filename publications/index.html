<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Micah Goldblum </title> <meta name="author" content="Micah Goldblum"> <meta name="description" content="Micah Goldblum. Math and ML. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://goldblum.github.io/publications/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Micah </span> Goldblum </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/website_cv.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/livebench-480.webp 480w,/assets/img/publication_preview/livebench-800.webp 800w,/assets/img/publication_preview/livebench-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/livebench.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="livebench.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="white2024livebench" class="col-sm-8"> <div class="title">Livebench: A challenging, contamination-free llm benchmark</div> <div class="author"> Colin White ,  Samuel Dooley ,  Manley Roberts ,  Arka Pal ,  Ben Feuer ,  Siddhartha Jain ,  Ravid Shwartz-Ziv ,  Neel Jain ,  Khalid Saifullah ,  Siddartha Naidu ,  Chinmay Hegde ,  Yann LeCun ,  Tom Goldstein ,  Willie Neiswanger ,  and  <em>Micah Goldblum</em> </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2406.19314" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Test set contamination, wherein test data from a benchmark ends up in a newer model’s training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 110B in size. LiveBench is difficult, with top models achieving below 65% accuracy. We release all questions, code, and model answers. Questions will be added and updated on a monthly basis, and we will release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/styleoutweighs-480.webp 480w,/assets/img/publication_preview/styleoutweighs-800.webp 800w,/assets/img/publication_preview/styleoutweighs-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/styleoutweighs.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="styleoutweighs.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="feuer2025style" class="col-sm-8"> <div class="title">Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking</div> <div class="author"> Benjamin Feuer ,  <em>Micah Goldblum</em> ,  Teresa Datta ,  Sanjana Nambiar ,  Raz Besaleli ,  Samuel Dooley ,  Max Cembalest ,  and  John P Dickerson </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2409.15268" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The release of ChatGPT in November 2022 sparked an explosion of interest in post-training and an avalanche of new preference optimization (PO) methods. These methods claim superior alignment by virtue of better correspondence with human pairwise preferences, often measured by LLM-judges. In this work, we attempt to answer the following question – do LLM-judge preferences translate to progress on other, more concrete metrics for alignment, and if not, why not? We define a concrete metric for alignment, and introduce SOS-Bench (Substance Outweighs Style Benchmark), which is to the best of our knowledge the largest standardized, reproducible LLM meta-benchmark to date. We find that (1) LLM-judge preferences do not correlate with concrete measures of safety, world knowledge, and instruction following; (2) LLM-judges have powerful implicit biases, prioritizing style over factuality and safety; and (3) the supervised fine-tuning (SFT) stage of post-training, and not the PO stage, has the greatest impact on alignment, with data scaling and prompt diversity as the driving factors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chencontinual-480.webp 480w,/assets/img/publication_preview/chencontinual-800.webp 800w,/assets/img/publication_preview/chencontinual-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/chencontinual.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chencontinual.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2025adaptive" class="col-sm-8"> <div class="title">Adaptive Rentention &amp; Correction for Continual Learning</div> <div class="author"> Haoran Chen ,  <em>Micah Goldblum</em> ,  Zuxuan Wu ,  and  Yu-Gang Jiang </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2405.14318" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Continual learning, also known as lifelong learning or incremental learning, refers to the process by which a model learns from a stream of incoming data over time. A common problem in continual learning is the classification layer’s bias towards the most recent task. Traditionally, methods have relied on incorporating data from past tasks during training to mitigate this issue. However, the recent shift in continual learning to memory-free environments has rendered these approaches infeasible. In this study, we propose a solution focused on the testing phase. We first introduce a simple Out-of-Task Detection method, OTD, designed to accurately identify samples from past tasks during testing. Leveraging OTD, we then propose: (1) an Adaptive Retention mechanism for dynamically tuning the classifier layer on past task data; (2) an Adaptive Correction mechanism for revising predictions when the model classifies data from previous tasks into classes from the current task. We name our approach Adaptive Retention &amp; Correction (ARC). While designed for memory-free environments, ARC also proves effective in memory-based settings. Extensive experiments show that our proposed method can be plugged in to virtually any existing continual learning approach without requiring any modifications to its training procedure. Specifically, when integrated with state-of-the-art approaches, ARC achieves an average performance increase of 2.7% and 2.6% on the CIFAR-100 and Imagenet-R datasets, respectively.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/hiddennomore-480.webp 480w,/assets/img/publication_preview/hiddennomore-800.webp 800w,/assets/img/publication_preview/hiddennomore-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/hiddennomore.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hiddennomore.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="thomashidden" class="col-sm-8"> <div class="title">Hidden No More: Attacking and Defending Private Third-Party LLM Inference</div> <div class="author"> Rahul Krishna Thomas ,  Louai Zahran ,  Erica Choi ,  Akilesh Potti ,  <em>Micah Goldblum</em> ,  and  Arka Pal </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=QfD9P9IIoz" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Recent advances in Large Language Models (LLMs) have led to widespread adoption of thirdparty inference services, raising critical privacy concerns. In this work, we introduce a novel reconstruction technique that can recover original prompts from hidden states with nearly perfect accuracy across multiple state-of-the-art LLMs in the increasingly important open-weights setting. Although the attack is conceptually simple, it has not– to the best of our knowledge– previously been described nor shown to work practically. Furthermore, our attack remains effective against various permutation and noise-based defenses, challenging assumptions about the security of previously proposed schemes. To address these vulnerabilities, we propose Cascade, a multiparty inference scheme that leverages sharding in the sequence dimension to retain privacy of the user input. Through theoretical analysis and empirical evaluation, we demonstrate that Cascade is secure against both our attack as well as previous methods, while maintaining computational and communication efficiency. Our findings highlight the importance of rigorous security analysis in privacy-preserving LLM inference and offer practical solutions for secure deployment.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/nofreelunch-480.webp 480w,/assets/img/publication_preview/nofreelunch-800.webp 800w,/assets/img/publication_preview/nofreelunch-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/nofreelunch.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nofreelunch.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="goldblum2024no" class="col-sm-8"> <div class="title">The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning</div> <div class="author"> <em>Micah Goldblum</em> ,  Marc Finzi ,  Keefer Rowan ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2304.05366" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/binoculars-480.webp 480w,/assets/img/publication_preview/binoculars-800.webp 800w,/assets/img/publication_preview/binoculars-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/binoculars.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="binoculars.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hans2024spotting" class="col-sm-8"> <div class="title">Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text</div> <div class="author"> Abhimanyu Hans ,  Avi Schwarzschild ,  Valeriia Cherepanova ,  Hamid Kazemi ,  Aniruddha Saha ,  <em>Micah Goldblum</em> ,  Jonas Geiping ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2401.12070" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/llmbound-480.webp 480w,/assets/img/publication_preview/llmbound-800.webp 800w,/assets/img/publication_preview/llmbound-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/llmbound.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="llmbound.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lotfi2024nonvacuous" class="col-sm-8"> <div class="title">Non-Vacuous Generalization Bounds for Large Language Models</div> <div class="author"> Sanae Lotfi ,  Marc Finzi ,  Yilun Kuang ,  Tim GJ Rudner ,  <em>Micah Goldblum</em> ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2312.17173" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Modern language models can contain billions of parameters, raising the question of whether they can generalize beyond the training data or simply regurgitate their training corpora. We provide the first non-vacuous generalization bounds for pretrained large language models (LLMs), indicating that language models are capable of discovering regularities that generalize to unseen data. In particular, we derive a compression bound that is valid for the unbounded log-likelihood loss using prediction smoothing, and we extend the bound to handle subsampling, accelerating bound computation on massive datasets. To achieve the extreme level of compression required for non-vacuous generalization bounds, we devise SubLoRA, a low-dimensional non-linear parameterization. Using this approach, we find that larger models have better generalization bounds and are more compressible than smaller models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/llmcalibration-480.webp 480w,/assets/img/publication_preview/llmcalibration-800.webp 800w,/assets/img/publication_preview/llmcalibration-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/llmcalibration.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="llmcalibration.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kapoor2024large" class="col-sm-8"> <div class="title">Large Language Models Must Be Taught to Know What They Don’t Know</div> <div class="author"> Sanyam Kapoor ,  Nate Gruver ,  Manley Roberts ,  Katherine Collins ,  Arka Pal ,  Umang Bhatt ,  Adrian Weller ,  Samuel Dooley ,  <em>Micah Goldblum</em> ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2406.08391" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>When using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. In this work, we first argue that prompting on its own is insufficient to achieve good calibration and then show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. We show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tunetables-480.webp 480w,/assets/img/publication_preview/tunetables-800.webp 800w,/assets/img/publication_preview/tunetables-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tunetables.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tunetables.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="feuer2024tunetables" class="col-sm-8"> <div class="title">TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks</div> <div class="author"> Benjamin Feuer ,  Robin Tibor Schirrmeister ,  Valeriia Cherepanova ,  Chinmay Hegde ,  Frank Hutter ,  <em>Micah Goldblum</em> ,  Niv Cohen ,  and  Colin White </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2402.11137" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel prompt-tuning strategy that compresses large datasets into a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datasets, while having a substantially lower inference time than TabPFN. Furthermore, we show that TuneTables can be used as an interpretability tool and can even be used to mitigate biases by optimizing a fairness objective.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/unlocking-480.webp 480w,/assets/img/publication_preview/unlocking-800.webp 800w,/assets/img/publication_preview/unlocking-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/unlocking.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="unlocking.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lotfi2024unlocking" class="col-sm-8"> <div class="title">Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models</div> <div class="author"> Sanae Lotfi ,  Yilun Kuang ,  Brandon Amos ,  <em>Micah Goldblum</em> ,  Marc Finzi ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2407.18158" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) with billions of parameters excel at predicting the next token in a sequence. Recent work computes non-vacuous compression-based generalization bounds for LLMs, but these bounds are vacuous for large models at the billion-parameter scale. Moreover, these bounds are obtained through restrictive compression techniques, bounding compressed models that generate low-quality text. Additionally, the tightness of these existing bounds depends on the number of IID documents in a training set rather than the much larger number of non-IID constituent tokens, leaving untapped potential for tighter bounds. In this work, we instead use properties of martingales to derive generalization bounds that benefit from the vast number of tokens in LLM training sets. Since a dataset contains far more tokens than documents, our generalization bounds not only tolerate but actually benefit from far less restrictive compression schemes. With Monarch matrices, Kronecker factorizations, and post-training quantization, we achieve non-vacuous generalization bounds for LLMs as large as LLaMA2-70B. Unlike previous approaches, our work achieves the first non-vacuous bounds for models that are deployed in practice and generate high-quality text.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/einsum-480.webp 480w,/assets/img/publication_preview/einsum-800.webp 800w,/assets/img/publication_preview/einsum-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/einsum.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="einsum.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="potapczynski2024searching" class="col-sm-8"> <div class="title">Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices</div> <div class="author"> Andres Potapczynski ,  Shikai Qiu ,  Marc Anton Finzi ,  Christopher Ferri ,  Zixi Chen ,  <em>Micah Goldblum</em> ,  Bayan C Bruss ,  Christopher De Sa ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://goldblum.github.io/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>Dense linear layers are the dominant computational bottleneck in large neural networks, presenting a critical need for more efficient alternatives. Previous efforts to develop alternatives have focused on a small number of hand-crafted structured matrices, and have neglected to investigate whether these structures can surpass dense layers in terms of compute-optimal scaling laws when both the model size and training examples are optimally allocated. In this work, we present a unifying framework that enables searching among all linear operators expressible via an Einstein summation. This framework encompasses many previously proposed structures, such as low-rank, Kronecker, Tensor-Train, and Monarch, along with many novel structures. We develop a taxonomy of all such operators based on their computational and algebraic properties, which provides insights into their scaling laws. Combining these insights with empirical evaluation, we identify a subset of structures that achieve equal or better performance than dense layers as a function of training compute. To further improve their compute efficiency, we develop a natural extension of these performant structures that convert them into a sparse Mixture-of-Experts layer. The resulting layer significantly outperforms dense layers in compute-optimal training efficiency for GPT-2 language models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/computebetterspent-480.webp 480w,/assets/img/publication_preview/computebetterspent-800.webp 800w,/assets/img/publication_preview/computebetterspent-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/computebetterspent.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="computebetterspent.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="qiu2024compute" class="col-sm-8"> <div class="title">Compute Better Spent: Replacing Dense Layers with Structured Matrices</div> <div class="author"> Shikai Qiu ,  Andres Potapczynski ,  Marc Anton Finzi ,  <em>Micah Goldblum</em> ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=ExHTFXEhc9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Dense linear layers are the dominant computational bottleneck in foundation models. Identifying more efficient alternatives to dense matrices has enormous potential for building more compute-efficient models, as exemplified by the success of convolutional networks in the image domain. In this work, we systematically explore structured matrices as replacements for dense matrices. We show that different structures often require drastically different initialization scales and learning rates, which are crucial to performance, especially as models scale. Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers. Finally, we measure the scaling laws of different structures to compare how quickly their performance improves with compute. We propose a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which we show performs better than dense matrices for the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less compute and is more efficient than dense for training small GPT-2 language models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/measuringstyle-480.webp 480w,/assets/img/publication_preview/measuringstyle-800.webp 800w,/assets/img/publication_preview/measuringstyle-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/measuringstyle.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="measuringstyle.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="somepalli2024measuring" class="col-sm-8"> <div class="title">Measuring Style Similarity in Diffusion Models</div> <div class="author"> Gowthami Somepalli ,  Anubhav Gupta ,  Kamal Gupta ,  Shramay Palta ,  <em>Micah Goldblum</em> ,  Jonas Geiping ,  Abhinav Shrivastava ,  and  Tom Goldstein </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2404.01292" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Generative models are now widely used by graphic designers and artists. Prior works have shown that these models remember and often replicate content from their training data during generation. Hence as their proliferation increases, it has become important to perform a database search to determine whether the properties of the image are attributable to specific training data, every time before a generated image is used for professional purposes. Existing tools for this purpose focus on retrieving images of similar semantic content. Meanwhile, many artists are concerned with style replication in text-to-image models. We present a framework for understanding and extracting style descriptors from images. Our framework comprises a new dataset curated using the insight that style is a subjective property of an image that captures complex yet meaningful interactions of factors including but not limited to colors, textures, shapes, etc. We also propose a method to extract style descriptors that can be used to attribute style of a generated image to the images used in the training dataset of a text-to-image model. We showcase promising results in various style retrieval tasks. We also quantitatively and qualitatively analyze style attribution and matching in the Stable Diffusion model.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/watermark-480.webp 480w,/assets/img/publication_preview/watermark-800.webp 800w,/assets/img/publication_preview/watermark-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/watermark.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="watermark.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kirchenbauer2024reliability" class="col-sm-8"> <div class="title">On the Reliability of Watermarks for Large Language Models</div> <div class="author"> John Kirchenbauer ,  Jonas Geiping ,  Yuxin Wen ,  Manli Shu ,  Khalid Saifullah ,  Kezhi Kong ,  Kasun Fernando ,  Aniruddha Saha ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=DEJIDCmWOz" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight our human study, where we investigate the reliability of watermarking when faced with human paraphrasing. We compare watermark-based detection to other detection strategies, finding overall that watermarking is a reliable solution, especially because of its sample complexity - for all attacks we consider, the watermark evidence compounds the more examples are given, and the watermark is eventually detected.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/neftune-480.webp 480w,/assets/img/publication_preview/neftune-800.webp 800w,/assets/img/publication_preview/neftune-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/neftune.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neftune.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jain2024neftune" class="col-sm-8"> <div class="title">NEFTune: Noisy Embeddings Improve Instruction Finetuning</div> <div class="author"> Neel Jain ,  Ping-yeh Chiang ,  Yuxin Wen ,  John Kirchenbauer ,  Hong-Min Chu ,  Gowthami Somepalli ,  Brian Bartoldson ,  Bhavya Kailkhura ,  Avi Schwarzschild ,  Aniruddha Saha ,  <em>Micah Goldblum</em> ,  Jonas Geiping ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=0bMmZ3fkCk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/guidance-480.webp 480w,/assets/img/publication_preview/guidance-800.webp 800w,/assets/img/publication_preview/guidance-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/guidance.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="guidance.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bansal2024universal" class="col-sm-8"> <div class="title">Universal guidance for diffusion models</div> <div class="author"> Arpit Bansal ,  Hong-Min Chu ,  Avi Schwarzschild ,  Soumyadip Sengupta ,  <em>Micah Goldblum</em> ,  Jonas Geiping ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=pzpWBbnwiJ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any use-specific components. We show that our algorithm successfully generates quality images with guidance functions including segmentation, face recognition, object detection, and classifier signals.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/backbone-480.webp 480w,/assets/img/publication_preview/backbone-800.webp 800w,/assets/img/publication_preview/backbone-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/backbone.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="backbone.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="goldblum2023battle" class="col-sm-8"> <div class="title">Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks</div> <div class="author"> <em>Micah Goldblum</em> ,  Hossein Souri ,  Renkun Ni ,  Manli Shu ,  Viraj Uday Prabhu ,  Gowthami Somepalli ,  Prithvijit Chattopadhyay ,  Adrien Bardes ,  Mark Ibrahim ,  Judy Hoffman ,  Rama Chellappa ,  Andrew Gordon Wilson ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2310.19909" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than 1500 training runs. While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. Moreover, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fairfacerec-480.webp 480w,/assets/img/publication_preview/fairfacerec-800.webp 800w,/assets/img/publication_preview/fairfacerec-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/fairfacerec.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fairfacerec.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dooley2023importance" class="col-sm-8"> <div class="title">Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition</div> <div class="author"> Samuel Dooley ,  Rhea Sukthanker ,  John P Dickerson ,  Colin White ,  Frank Hutter ,  and  <em>Micah Goldblum</em> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2210.09943" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race. Conventional wisdom dictates that model biases arise from biased training data. As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition. In our work, we discover that biases are actually inherent to neural network architectures themselves. Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairness, often by large margins, on the two most widely used datasets for face identification, CelebA and VGGFace2. Furthermore, these models generalize to other datasets and sensitive attributes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lasso-480.webp 480w,/assets/img/publication_preview/lasso-800.webp 800w,/assets/img/publication_preview/lasso-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/lasso.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lasso.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cherepanova2023performance" class="col-sm-8"> <div class="title">A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning</div> <div class="author"> Valeriia Cherepanova ,  Gowthami Somepalli ,  Jonas Geiping ,  C. Bayan Bruss ,  Andrew Gordon Wilson ,  Tom Goldstein ,  and  <em>Micah Goldblum</em> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2311.05877" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Academic tabular benchmarks often contain small sets of curated features. In contrast, data scientists typically collect as many features as possible into their datasets, and even engineer new features from existing ones. To prevent over-fitting in subsequent downstream modeling, practitioners commonly use automated feature selection methods that identify a reduced subset of informative features. Existing benchmarks for tabular feature selection consider classical downstream models, toy synthetic datasets, or do not evaluate feature selectors on the basis of downstream performance. We construct a challenging feature selection benchmark evaluated on downstream neural networks including transformers, using real datasets and multiple methods for generating extraneous features. We also propose an input-gradient-based analogue of LASSO for neural networks that outperforms classical feature selection methods on challenging problems such as selecting from corrupted or second-order features.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/imbalance-480.webp 480w,/assets/img/publication_preview/imbalance-800.webp 800w,/assets/img/publication_preview/imbalance-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/imbalance.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="imbalance.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schwartz2023simplifying" class="col-sm-8"> <div class="title">Simplifying Neural Network Training Under Class Imbalance</div> <div class="author"> Ravid Shwartz-Ziv ,  <em>Micah Goldblum</em> ,  Yucen Lily Li ,  C. Bayan Bruss ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2312.02517" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Real-world datasets are often highly class-imbalanced, which can adversely impact the performance of deep learning models. The majority of research on training neural networks under class imbalance has focused on specialized loss functions and sampling techniques. Notably, we demonstrate that simply tuning existing components of standard deep learning pipelines, such as the batch size, data augmentation, architecture size, pre-training, optimizer, and label smoothing, can achieve state-of-the-art performance without any specialized loss functions or samplers. We also provide key prescriptions and considerations for training under class imbalance, and an understanding of why imbalance methods succeed or fail.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/colddiffusion-480.webp 480w,/assets/img/publication_preview/colddiffusion-800.webp 800w,/assets/img/publication_preview/colddiffusion-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/colddiffusion.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="colddiffusion.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bansal2023cold" class="col-sm-8"> <div class="title">Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise</div> <div class="author"> Arpit Bansal ,  Eitan Borgnia ,  Hong-Min Chu ,  Jie S Li ,  Hamid Kazemi ,  Furong Huang ,  <em>Micah Goldblum</em> ,  Jonas Geiping ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2208.09392" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Standard diffusion models involve an image transform – adding Gaussian noise – and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community’s understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mitigatecopying-480.webp 480w,/assets/img/publication_preview/mitigatecopying-800.webp 800w,/assets/img/publication_preview/mitigatecopying-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mitigatecopying.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mitigatecopying.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="somepalli2023diffusion" class="col-sm-8"> <div class="title">Why Diffusion Models Memorize and How to Mitigate Copying</div> <div class="author"> Gowthami Somepalli ,  Vasu Singla ,  <em>Micah Goldblum</em> ,  Jonas Geiping ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2305.20086" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p> Images generated by diffusion models like Stable Diffusion are increasingly widespread. Recent works and even lawsuits have shown that these models are prone to replicating their training data, unbeknownst to the user. In this paper, we first analyze this memorization problem in text-to-image diffusion models. While it is widely believed that duplicated images in the training set are responsible for content replication at inference time, we observe that the text conditioning of the model plays a similarly important role. In fact, we see in our experiments that data replication often does not happen for unconditional models, while it is common in the text-conditional case. Motivated by our findings, we then propose several techniques for reducing data replication at inference time by randomizing and augmenting image captions in the training set.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/hardprompts-480.webp 480w,/assets/img/publication_preview/hardprompts-800.webp 800w,/assets/img/publication_preview/hardprompts-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/hardprompts.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hardprompts.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wen2023hard" class="col-sm-8"> <div class="title">Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery</div> <div class="author"> Yuxin Wen ,  Neel Jain ,  John Kirchenbauer ,  <em>Micah Goldblum</em> ,  Jonas Geiping ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2302.03668" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical "hard" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also "soft" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tree-480.webp 480w,/assets/img/publication_preview/tree-800.webp 800w,/assets/img/publication_preview/tree-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tree.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tree.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mcelfresh2023neural" class="col-sm-8"> <div class="title">When Do Neural Nets Outperform Boosted Trees on Tabular Data?</div> <div class="author"> Duncan McElfresh ,  Sujay Khandagale ,  Jonathan Valverde ,  Ganesh Ramakrishnan ,  <em>Micah Goldblum</em> ,  Colin White ,  and  others </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2305.02997" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, ’does it matter?’ We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the ’NN vs. GBDT’ debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed feature distributions, heavy-tailed feature distributions, and other forms of dataset irregularities. Our insights act as a guide for practitioners to decide whether or not they need to run a neural net to reach top performance on their dataset.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/unlearnable-480.webp 480w,/assets/img/publication_preview/unlearnable-800.webp 800w,/assets/img/publication_preview/unlearnable-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/unlearnable.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="unlearnable.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sandoval2023can" class="col-sm-8"> <div class="title">What Can We Learn from Unlearnable Datasets</div> <div class="author"> Pedro Sandoval-Segura ,  Vasu Singla ,  Jonas Geiping ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2305.19254" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>In an era of widespread web scraping, unlearnable dataset methods have the potential to protect data privacy by preventing deep neural networks from generalizing. But in addition to a number of practical limitations that make their use unlikely, we make a number of findings that call into question their ability to safeguard data. First, it is widely believed that neural networks trained on unlearnable datasets only learn shortcuts, simpler rules that are not useful for generalization. In contrast, we find that networks actually can learn useful features that can be reweighed for high test performance, suggesting that image privacy is not preserved. Unlearnable datasets are also believed to induce learning shortcuts through linear separability of added perturbations. We provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition. To emphasize why linearly separable perturbations should not be relied upon, we propose an orthogonal projection attack which allows learning from unlearnable datasets published in ICML 2021 and ICLR 2023. Our proposed attack is significantly less complex than recently proposed techniques.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tabular-480.webp 480w,/assets/img/publication_preview/tabular-800.webp 800w,/assets/img/publication_preview/tabular-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tabular.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tabular.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="levin2023transfer" class="col-sm-8"> <div class="title">Transfer Learning with Deep Tabular Models</div> <div class="author"> Roman Levin ,  Valeriia Cherepanova ,  Avi Schwarzschild ,  Arpit Bansal ,  Bayan Bruss ,  Tom Goldstein ,  Andrew Gordon Wilson ,  and  <em>Micah Goldblum</em> </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=b0RuGUYo8pA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they are easily fine-tuned in new domains and learn reusable features. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we explore the benefits that representation learning provides for knowledge transfer in the tabular domain. We conduct experiments in a realistic medical diagnosis test bed with limited amounts of downstream data and find that transfer learning with deep tabular models provides a definitive advantage over gradient boosted decision tree methods. We further compare the supervised and self-supervised pretraining strategies and provide practical advice on transfer learning with tabular models. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/target-480.webp 480w,/assets/img/publication_preview/target-800.webp 800w,/assets/img/publication_preview/target-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/target.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="target.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chiang2023gradient" class="col-sm-8"> <div class="title">Gradient-Based Optimization Is Not Necessary for Generalization in Neural Networks</div> <div class="author"> Ping-yeh Chiang ,  Renkun Ni ,  David Yu Miller ,  Arpit Bansal ,  Jonas Geiping ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=QC10RmRbZy9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>It is commonly believed that the implicit regularization of optimizers is needed for neural networks to generalize in the overparameterized regime. In this paper, we observe experimentally that this implicit regularization behavior is \em generic, i.e. it does not depend strongly on the choice of optimizer. We demonstrate this by training neural networks using several gradient-free optimizers that do not benefit from properties that are often attributed to gradient-based optimizers. This includes a guess-and-check optimizer that generates uniformly random parameter vectors until one is found that happens to achieve perfect train accuracy, and a zeroth-order pattern search optimizer that uses no gradient computations. In the low sample and few-shot regimes, where zeroth order optimizers are most tractable, we find that these non-gradient optimizers achieve test accuracy comparable to SGD.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/decision_boundaries-480.webp 480w,/assets/img/publication_preview/decision_boundaries-800.webp 800w,/assets/img/publication_preview/decision_boundaries-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/decision_boundaries.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="decision_boundaries.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2023exploring" class="col-sm-8"> <div class="title">Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness</div> <div class="author"> Yuancheng Xu ,  Yanchao Sun ,  <em>Micah Goldblum</em> ,  Tom Goldstein ,  and  Furong Huang </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=aRTKuscKByJ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The robustness of a deep classifier can be characterized by its margins: the decision boundary’s distances to natural data points. However, it is unclear whether existing robust training methods effectively increase the margin for each vulnerable point during training. To understand this, we propose a continuous-time framework for quantifying the relative speed of the decision boundary with respect to each individual point. Through visualizing the moving speed of the decision boundary under Adversarial Training, one of the most effective robust training algorithms, a surprising moving-behavior is revealed: the decision boundary moves away from some vulnerable points but simultaneously moves closer to others, decreasing their margins. To alleviate these conflicting dynamics of the decision boundary, we propose Dynamics-aware Robust Training (DyART), which encourages the decision boundary to engage in movement that prioritizes increasing smaller margins. In contrast to prior works, DyART directly operates on the margins rather than their indirect approximations, allowing for more targeted and effective robustness improvement. Experiments on the CIFAR-10 and Tiny-ImageNet datasets verify that DyART alleviates the conflicting dynamics of the decision boundary and obtains improved robustness under various perturbation sizes compared to the state-of-the-art defenses. Our code is available at https://github.com/Yuancheng-Xu/Dynamics-Aware-Robust-Training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/canary-480.webp 480w,/assets/img/publication_preview/canary-800.webp 800w,/assets/img/publication_preview/canary-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/canary.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="canary.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wen2023canary" class="col-sm-8"> <div class="title">Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries</div> <div class="author"> Yuxin Wen ,  Arpit Bansal ,  Hamid Kazemi ,  Eitan Borgnia ,  <em>Micah Goldblum</em> ,  Jonas Geiping ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=b7SBTEBFnC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>As industrial applications are increasingly automated by machine learning models, enforcing personal data ownership and intellectual property rights requires tracing training data back to their rightful owners. Membership inference algorithms approach this problem by using statistical techniques to discern whether a target sample was included in a model’s training set. However, existing methods only utilize the unaltered target sample or simple augmentations of the target to compute statistics. Such a sparse sampling of the model’s behavior carries little information, leading to poor inference capabilities. In this work, we use adversarial tools to directly optimize for queries that are discriminative and diverse. Our improvements achieve significantly more accurate membership inference than existing methods, especially in offline scenarios and in the low false-positive regime which is critical in legal settings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lie-480.webp 480w,/assets/img/publication_preview/lie-800.webp 800w,/assets/img/publication_preview/lie-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/lie.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lie.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gruver2023lie" class="col-sm-8"> <div class="title">The Lie Derivative for Measuring Learned Equivariance</div> <div class="author"> Nate Gruver ,  Marc Anton Finzi ,  <em>Micah Goldblum</em> ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=JL7Va5Vy15J" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Equivariance guarantees that a model’s predictions capture key symmetries in data. When an image is translated or rotated, an equivariant model’s representation of that image will translate or rotate accordingly. The success of convolutional neural networks has historically been tied to their ability to directly encode translation equivariance in their architecture. The rising success of vision transformers, which have no explicit architectural bias towards equivariance, challenges this narrative and suggests that augmentations and training data might also play a significant role in their performance. In order to better understand the role of equivariance in recent vision models, we introduce the Lie derivative, a method for measuring equivariance with strong mathematical foundations and minimal hyperparameters. Using the Lie derivative, we study the equivariance properties of hundreds of pretrained models, spanning CNNs, transformers, and Mixer architectures. The scale of our analysis allows us to separate the impact of architecture from other factors like model size or training method. Surprisingly, we find that many violations of equivariance can be linked to spatial aliasing in ubiquitous network layers, such as pointwise non-linearities, and that as models get larger and more accurate they tend to display more equivariance, regardless of architecture.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/panning-480.webp 480w,/assets/img/publication_preview/panning-800.webp 800w,/assets/img/publication_preview/panning-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/panning.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="panning.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chu2023panning" class="col-sm-8"> <div class="title">Panning for Gold in Federated Learning: Targeted Text Extraction under Arbitrarily Large-Scale Aggregation</div> <div class="author"> Hong-Min Chu ,  Jonas Geiping ,  Liam H Fowl ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=A9WQaxYsfx" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>As federated learning (FL) matures, privacy attacks against FL systems in turn become more numerous and complex. Attacks on language models have progressed from recovering single sentences in simple classification tasks to recovering larger parts of user data. Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence. Because of this, current attacks fail to extract any meaningful data under large-scale aggregation. In realistic settings, an attacker cares most about a small portion of user data that contains sensitive personal information, for example sequences containing the phrase "my credit card number is ...". In this work, we propose the first attack on FL that achieves targeted extraction of sequences that contain privacy-critical phrases, whereby we employ maliciously modified parameters to allow the transformer itself to filter relevant sequences from aggregated user data and encode them in the gradient update. Our attack can effectively extract sequences of interest even against extremely large-scale aggregation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/augmentation-480.webp 480w,/assets/img/publication_preview/augmentation-800.webp 800w,/assets/img/publication_preview/augmentation-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/augmentation.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="augmentation.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="geiping2023much" class="col-sm-8"> <div class="title">How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization</div> <div class="author"> Jonas Geiping ,  <em>Micah Goldblum</em> ,  Gowthami Somepalli ,  Ravid Shwartz-Ziv ,  Tom Goldstein ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=3aQs3MCSexD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Despite the clear performance benefits of data augmentations, little is known about why they are so effective. In this paper, we disentangle several key mechanisms through which data augmentations operate. Establishing an exchange rate between augmented and additional real data, we find that in out-of-distribution testing scenarios, augmentations which yield samples that are diverse, but inconsistent with the data distribution can be even more valuable than additional training data. Moreover, we find that data augmentations which encourage invariances can be more valuable than invariance alone, especially on small and medium sized training sets. Following this observation, we show that augmentations induce additional stochasticity during training, effectively flattening the loss landscape.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/decepticons-480.webp 480w,/assets/img/publication_preview/decepticons-800.webp 800w,/assets/img/publication_preview/decepticons-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/decepticons.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="decepticons.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="fowl2023decepticons" class="col-sm-8"> <div class="title">Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models</div> <div class="author"> Liam H Fowl ,  Jonas Geiping ,  Steven Reich ,  Yuxin Wen ,  Wojciech Czaja ,  Goldblum. Micah ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=r0BrY4BiEXO" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/wave-480.webp 480w,/assets/img/publication_preview/wave-800.webp 800w,/assets/img/publication_preview/wave-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/wave.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="wave.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="somepalli2023diffusioo" class="col-sm-8"> <div class="title">Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models</div> <div class="author"> Gowthami Somepalli ,  Vasu Singla ,  <em>Micah Goldblum</em> ,  Jonas Geiping ,  and  Tom Goldstein </div> <div class="periodical"> <em>Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2212.03860" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cookbook-480.webp 480w,/assets/img/publication_preview/cookbook-800.webp 800w,/assets/img/publication_preview/cookbook-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/cookbook.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cookbook.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="balestriero2023cookbook" class="col-sm-8"> <div class="title">A cookbook of self-supervised learning</div> <div class="author"> Randall Balestriero ,  Mark Ibrahim ,  Vlad Sobal ,  Ari Morcos ,  Shashank Shekhar ,  Tom Goldstein ,  Florian Bordes ,  Adrien Bardes ,  Gregoire Mialon ,  Yuandong Tian ,  Avi Schwarzschild ,  Andrew Wilson ,  Jonas Geiping ,  Quentin Garrido ,  Pierre Fernandez ,  Amir Bar ,  Hamed Pirsiavash ,  Yann LeCun ,  and  <em>Micah Goldblum</em> </div> <div class="periodical"> <em>Preprint</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2304.12210" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/marginal-480.webp 480w,/assets/img/publication_preview/marginal-800.webp 800w,/assets/img/publication_preview/marginal-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/marginal.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="marginal.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lotfi2022bayesian" class="col-sm-8"> <div class="title">Bayesian Model Selection, the Marginal Likelihood, and Generalization</div> <div class="author"> Sanae Lotfi ,  Pavel Izmailov ,  Gregory Benton ,  <em>Micah Goldblum</em> ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML) Outstanding Paper Award</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2202.11678" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam’s razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/backdoor-480.webp 480w,/assets/img/publication_preview/backdoor-800.webp 800w,/assets/img/publication_preview/backdoor-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/backdoor.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="backdoor.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="goldblum2022dataset" class="col-sm-8"> <div class="title">Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses</div> <div class="author"> <em>Micah Goldblum</em> ,  Dimitris Tsipras ,  Chulin Xie ,  Xinyun Chen ,  Avi Schwarzschild ,  Dawn Song ,  Aleksander Madry ,  Bo Li ,  and  Tom Goldstein </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2012.10544" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space. In addition to describing various poisoning and backdoor threat models and the relationships among them, we develop their unified taxonomy.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/recurrence-480.webp 480w,/assets/img/publication_preview/recurrence-800.webp 800w,/assets/img/publication_preview/recurrence-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/recurrence.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="recurrence.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schwarzschild2021uncanny" class="col-sm-8"> <div class="title">The Uncanny Similarity of Recurrence and Depth</div> <div class="author"> Avi Schwarzschild ,  Arjun Gupta ,  Amin Ghiasi ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2102.11011" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>It is widely believed that deep neural networks contain layer specialization, wherein networks extract hierarchical features representing edges and patterns in shallow layers and complete objects in deeper layers. Unlike common feed-forward models that have distinct filters at each layer, recurrent networks reuse the same parameters at various depths. In this work, we observe that recurrent models exhibit the same hierarchical behaviors and the same performance benefits as depth despite reusing the same filters at every recurrence. By training models of various feed-forward and recurrent architectures on several datasets for image classification as well as maze solving, we show that recurrent networks have the ability to closely emulate the behavior of non-recurrent deep models, often doing so with far fewer parameters.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/metacontrastive-480.webp 480w,/assets/img/publication_preview/metacontrastive-800.webp 800w,/assets/img/publication_preview/metacontrastive-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/metacontrastive.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="metacontrastive.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ni2022contrastive" class="col-sm-8"> <div class="title">The Close Relationship Between Contrastive Learning and Meta-Learning</div> <div class="author"> Renkun Ni ,  Manli Shu ,  Hossein Souri ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=gICys3ITSmj" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Contrastive learning has recently taken off as a paradigm for learning from unlabeled data. In this paper, we discuss the close relationship between contrastive learning and meta-learning, and in fact show that contrastive learning can be interpreted as a special case of meta-learning with a certain task distribution. We complement this observation by showing that established meta-learning methods, such as Prototypical Networks, achieve comparable performance to SimCLR when paired with this task distribution. This close relationship can be leveraged by taking established techniques from the meta-learning literature, such as task-based data augmentation, and showing that they benefit contrastive learning as well. These tricks also benefit state-of-the-art self-supervised learners without using negative pairs such as BYOL, which achieves 94.6% accuracy on CIFAR-10 using a self-supervised ResNet-18 feature extractor trained with our meta-learning tricks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/stochastic-480.webp 480w,/assets/img/publication_preview/stochastic-800.webp 800w,/assets/img/publication_preview/stochastic-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/stochastic.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="stochastic.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="geiping2022stochastic" class="col-sm-8"> <div class="title">Stochastic Training is Not Necessary for Generalization</div> <div class="author"> Jonas Geiping ,  <em>Micah Goldblum</em> ,  Phil Pope ,  Michael Moeller ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2109.14119" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>It is widely believed that the implicit regularization of SGD is fundamental to the impressive generalization behavior we observe in neural networks. In this work, we demonstrate that non-stochastic full-batch training can achieve comparably strong performance to SGD on CIFAR-10 using modern architectures. To this end, we show that the implicit regularization of SGD can be completely replaced with explicit regularization. Our observations indicate that the perceived difficulty of full-batch training is largely the result of its optimization properties and the disproportionate time and effort spent by the ML community tuning optimizers and hyperparameters for small-batch training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/robber-480.webp 480w,/assets/img/publication_preview/robber-800.webp 800w,/assets/img/publication_preview/robber-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/robber.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="robber.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="fowl2022robbing" class="col-sm-8"> <div class="title">Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models</div> <div class="author"> Liam Fowl ,  Jonas Geiping ,  Wojciech Czaja ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2110.13057" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Federated learning has quickly gained popularity with its promises of increased user privacy and efficiency. Previous works have shown that federated gradient updates contain information that can be used to approximately recover user data in some situations. These previous attacks on user privacy have been limited in scope and do not scale to gradient updates aggregated over even a handful of data points, leaving some to conclude that data privacy is still intact for realistic training regimes. In this work, we introduce a new threat model based on minimal but malicious modifications of the shared model architecture which enable the server to directly obtain a verbatim copy of user data from gradient updates without solving difficult inverse problems. Even user data aggregated over large batches – where previous methods fail to extract meaningful content – can be reconstructed by these minimally modified models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fishing-480.webp 480w,/assets/img/publication_preview/fishing-800.webp 800w,/assets/img/publication_preview/fishing-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/fishing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fishing.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wen2022fishing" class="col-sm-8"> <div class="title">Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification</div> <div class="author"> Yuxin Wen ,  Jonas Geiping ,  Liam Fowl ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2202.00580" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Federated learning (FL) has rapidly risen in popularity due to its promise of privacy and efficiency. Previous works have exposed privacy vulnerabilities in the FL pipeline by recovering user data from gradient updates. However, existing attacks fail to address realistic settings because they either 1) require toy settings with very small batch sizes, or 2) require unrealistic and conspicuous architecture modifications. We introduce a new strategy that dramatically elevates existing attacks to operate on batches of arbitrarily large size, and without architectural modifications. Our model-agnostic strategy only requires modifications to the model parameters sent to the user, which is a realistic threat model in many scenarios. We demonstrate the strategy in challenging large-scale settings, obtaining high-fidelity data extraction in both cross-device and cross-silo federated learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/plugininversion-480.webp 480w,/assets/img/publication_preview/plugininversion-800.webp 800w,/assets/img/publication_preview/plugininversion-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/plugininversion.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="plugininversion.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ghiasi2022plug" class="col-sm-8"> <div class="title">Plug-In Inversion: Model-Agnostic Inversion for Vision with Data Augmentations</div> <div class="author"> Amin Ghiasi ,  Hamid Kazemi ,  Steven Reich ,  Chen Zhu ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2201.12961" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Existing techniques for model inversion typically rely on hard-to-tune regularizers, such as total variation or feature regularization, which must be individually calibrated for each network in order to produce adequate images. In this work, we introduce Plug-In Inversion, which relies on a simple set of augmentations and does not require excessive hyper-parameter tuning. Under our proposed augmentation-based scheme, the same set of augmentation hyper-parameters can be used for inverting a wide range of image classification models, regardless of input dimensions or the architecture. We illustrate the practicality of our approach by inverting Vision Transformers (ViTs) and Multi-Layer Perceptrons (MLPs) trained on the ImageNet dataset, tasks which to the best of our knowledge have not been successfully accomplished by any previous works.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/boundaries-480.webp 480w,/assets/img/publication_preview/boundaries-800.webp 800w,/assets/img/publication_preview/boundaries-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/boundaries.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="boundaries.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="somepalli2022can" class="col-sm-8"> <div class="title">Can You Learn the Same Model Twice? Investigating Reproducibility and Double Descent from the Decision Boundary Perspective</div> <div class="author"> Gowthami Somepalli ,  Liam Fowl ,  Arpit Bansal ,  Ping Ye-Chiang ,  Yehuda Dar ,  Richard Baraniuk ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2203.08124" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We discuss methods for visualizing neural network decision boundaries and decision regions. We use these visualizations to investigate issues related to reproducibility and generalization in neural network training. We observe that changes in model architecture (and its associate inductive bias) cause visible changes in decision boundaries, while multiple runs with the same architecture yield results with strong similarities, especially in the case of wide architectures. We also use decision boundary methods to visualize double descent phenomena. We see that decision boundary reproducibility depends strongly on model width. Near the threshold of interpolation, neural network decision boundaries become fragmented into many small decision regions, and these regions are non-reproducible. Meanwhile, very narrows and very wide networks have high levels of reproducibility in their decision boundaries with relatively few decision regions. We discuss how our observations relate to the theory of double descent phenomena in convex models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/autoregressive_50-480.webp 480w,/assets/img/publication_preview/autoregressive_50-800.webp 800w,/assets/img/publication_preview/autoregressive_50-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/autoregressive_50.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="autoregressive_50.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sandoval2022autoregressive" class="col-sm-8"> <div class="title">Autoregressive Perturbations for Data Poisoning</div> <div class="author"> Pedro Sandoval-Segura ,  Vasu Singla ,  Jonas Geiping ,  <em>Micah Goldblum</em> ,  Tom Goldstein ,  and  David W Jacobs </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2206.03693" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The prevalence of data scraping from social media as a means to obtain datasets has led to growing concerns regarding unauthorized use of data. Data poisoning attacks have been proposed as a bulwark against scraping, as they make data "unlearnable" by adding small, imperceptible perturbations. Unfortunately, existing methods require knowledge of both the target architecture and the complete dataset so that a surrogate network can be trained, the parameters of which are used to generate the attack. In this work, we introduce autoregressive (AR) poisoning, a method that can generate poisoned data without access to the broader dataset. The proposed AR perturbations are generic, can be applied across different datasets, and can poison different architectures. Compared to existing unlearnable methods, our AR poisons are more resistant against common defenses such as adversarial training and strong data augmentations. Our analysis further provides insight into what makes an effective data poison.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pretrain_30-480.webp 480w,/assets/img/publication_preview/pretrain_30-800.webp 800w,/assets/img/publication_preview/pretrain_30-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/pretrain_30.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pretrain_30.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shwartz2022pre" class="col-sm-8"> <div class="title">Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors</div> <div class="author"> Ravid Shwartz-Ziv ,  <em>Micah Goldblum</em> ,  Hossein Souri ,  Sanyam Kapoor ,  Chen Zhu ,  Yann LeCun ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2205.10279" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/maze-480.webp 480w,/assets/img/publication_preview/maze-800.webp 800w,/assets/img/publication_preview/maze-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/maze.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="maze.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bansal2022end" class="col-sm-8"> <div class="title">End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking</div> <div class="author"> Arpit Bansal ,  Avi Schwarzschild ,  Eitan Borgnia ,  Zeyad Emam ,  Furong Huang ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2202.05826" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Machine learning systems perform well on pattern matching tasks, but their ability to perform algorithmic or logical reasoning is not well understood. One important reasoning capability is algorithmic extrapolation, in which models trained only on small/simple reasoning problems can synthesize complex strategies for large/complex problems at test time. Algorithmic extrapolation can be achieved through recurrent systems, which can be iterated many times to solve difficult reasoning problems. We observe that this approach fails to scale to highly complex problems because behavior degenerates when many iterations are applied – an issue we refer to as "overthinking." We propose a recall architecture that keeps an explicit copy of the problem instance in memory so that it cannot be forgotten. We also employ a progressive training routine that prevents the model from learning behaviors that are specific to iteration number and instead pushes it to learn behaviors that can be repeated indefinitely. These innovations prevent the overthinking problem, and enable recurrent systems to solve extremely hard extrapolation tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sleeper-480.webp 480w,/assets/img/publication_preview/sleeper-800.webp 800w,/assets/img/publication_preview/sleeper-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/sleeper.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sleeper.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="souri2022sleeper" class="col-sm-8"> <div class="title">Sleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch</div> <div class="author"> Hossein Souri ,  <em>Micah Goldblum</em> ,  Liam Fowl ,  Rama Chellappa ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2106.08970" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>As the curation of data for machine learning becomes increasingly automated, dataset tampering is a mounting threat. Backdoor attackers tamper with training data to embed a vulnerability in models that are trained on that data. This vulnerability is then activated at inference time by placing a "trigger" into the model’s input. Typical backdoor attacks insert the trigger directly into the training data, although the presence of such an attack may be visible upon inspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning without placing a trigger into the training data at all. However, this hidden trigger attack is ineffective at poisoning neural networks trained from scratch. We develop a new hidden trigger attack, Sleeper Agent, which employs gradient matching, data selection, and target model re-training during the crafting process. Sleeper Agent is the first hidden trigger backdoor attack to be effective against neural networks trained from scratch. We demonstrate its effectiveness on ImageNet and in black-box settings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/explainable_50-480.webp 480w,/assets/img/publication_preview/explainable_50-800.webp 800w,/assets/img/publication_preview/explainable_50-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/explainable_50.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="explainable_50.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="levin2022models" class="col-sm-8"> <div class="title">Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability</div> <div class="author"> Roman Levin ,  Manli Shu ,  Eitan Borgnia ,  Furong Huang ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2108.01335" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Conventional saliency maps highlight input features to which neural network predictions are highly sensitive. We take a different approach to saliency, in which we identify and analyze the network parameters, rather than inputs, which are responsible for erroneous decisions. We find that samples which cause similar parameters to malfunction are semantically similar. We also show that pruning the most salient parameters for a wrongly classified sample often improves model behavior. Furthermore, fine-tuning a small number of the most salient parameters on a single sample results in error correction on other samples that are misclassified for similar reasons. Based on our parameter saliency method, we also introduce an input-space saliency technique that reveals how image features cause specific network components to malfunction. Further, we rigorously validate the meaningfulness of our saliency maps on both the dataset and case-study levels.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/shortcut-480.webp 480w,/assets/img/publication_preview/shortcut-800.webp 800w,/assets/img/publication_preview/shortcut-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/shortcut.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="shortcut.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2022chroma" class="col-sm-8"> <div class="title">Chroma-VAE: Mitigating Shortcut Learning with Generative Classifiers</div> <div class="author"> Wanqian Yang ,  Polina Kirichenko ,  <em>Micah Goldblum</em> ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=WWVcsfI0jGH" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Deep neural networks are susceptible to shortcut learning, using simple features to achieve low training loss without discovering essential semantic structure. Contrary to prior belief, we show that generative models alone are not sufficient to prevent shortcut learning, despite an incentive to recover a more comprehensive representation of the data than discriminative approaches. However, we observe that shortcuts are preferentially encoded with minimal information, a fact that generative models can exploit to mitigate shortcut learning. In particular, we propose Chroma-VAE, a two-pronged approach where a VAE classifier is initially trained to isolate the shortcut in a small latent subspace, allowing a secondary classifier to be trained on the complementary, shortcut-free latent subspace. In addition to demonstrating the efficacy of Chroma-VAE on benchmark and real-world shortcut learning tasks, our work highlights the potential for manipulating the latent space of generative classifiers to isolate or interpret specific correlations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/compression_50-480.webp 480w,/assets/img/publication_preview/compression_50-800.webp 800w,/assets/img/publication_preview/compression_50-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/compression_50.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="compression_50.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lotfi2022pac" class="col-sm-8"> <div class="title">PAC-Bayes Compression Bounds So Tight That They Can Explain Generalization</div> <div class="author"> Sanae Lotfi ,  Marc Anton Finzi ,  Sanyam Kapoor ,  Andres Potapczynski ,  <em>Micah Goldblum</em> ,  and  Andrew Gordon Wilson </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=o8nYuR8ekFm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>While there has been progress in developing non-vacuous generalization bounds for deep neural networks, these bounds tend to be uninformative about why deep learning works. In this paper, we develop a compression approach based on quantizing neural network parameters in a linear subspace, profoundly improving on previous results to provide state-of-the-art generalization bounds on a variety of tasks, including transfer learning. We use these tight bounds to better understand the role of model size, equivariance, and the implicit biases of optimization, for generalization in deep learning. Notably, we find large models can be compressed to a much greater extent than previously known, encapsulating Occam’s razor.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/deepthink1-480.webp 480w,/assets/img/publication_preview/deepthink1-800.webp 800w,/assets/img/publication_preview/deepthink1-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/deepthink1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="deepthink1.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schwarzschild2021can" class="col-sm-8"> <div class="title">Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks</div> <div class="author"> Avi Schwarzschild ,  Eitan Borgnia ,  Arjun Gupta ,  Furong Huang ,  Uzi Vishkin ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2106.04537" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Deep neural networks are powerful machines for visual pattern recognition, but reasoning tasks that are easy for humans may still be difficult for neural models. Humans possess the ability to extrapolate reasoning strategies learned on simple problems to solve harder examples, often by thinking for longer. For example, a person who has learned to solve small mazes can easily extend the very same search techniques to solve much larger mazes by spending more time. In computers, this behavior is often achieved through the use of algorithms, which scale to arbitrarily hard problem instances at the cost of more computation. In contrast, the sequential computing budget of feed-forward neural networks is limited by their depth, and networks trained on simple problems have no way of extending their reasoning to accommodate harder problems. In this work, we show that recurrent networks trained to solve simple problems with few recurrent steps can indeed solve much more complex problems simply by performing additional recurrences during inference. We demonstrate this algorithmic behavior of recurrent networks on prefix sum computation, mazes, and chess. In all three domains, networks trained on simple problem instances are able to extend their reasoning abilities at test time simply by "thinking for longer."</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/advbn-480.webp 480w,/assets/img/publication_preview/advbn-800.webp 800w,/assets/img/publication_preview/advbn-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/advbn.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="advbn.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shu2020prepare" class="col-sm-8"> <div class="title">Prepare for the Worst: Generalizing across Domain Shifts with Adversarial Batch Normalization</div> <div class="author"> Manli Shu ,  Zuxuan Wu ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2009.08965" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Adversarial training is the industry standard for producing models that are robust to small adversarial perturbations. However, machine learning practitioners need models that are robust to domain shifts that occur naturally, such as changes in the style or illumination of input images. Such changes in input distribution have been effectively modeled as shifts in the mean and variance of deep image features. We adapt adversarial training by adversarially perturbing these feature statistics, rather than image pixels, to produce models that are robust to domain shift. We also visualize images from adversarially crafted distributions. Our method, Adversarial Batch Normalization (AdvBN), significantly improves the performance of ResNet-50 on ImageNet-C (+8.1%), Stylized-ImageNet (+6.7%), and ImageNet-Instagram (+3.9%) over standard training practices. In addition, we demonstrate that AdvBN can also improve generalization on semantic segmentation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/adversarial_poisoning-480.webp 480w,/assets/img/publication_preview/adversarial_poisoning-800.webp 800w,/assets/img/publication_preview/adversarial_poisoning-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/adversarial_poisoning.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="adversarial_poisoning.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="fowl2021adversarial" class="col-sm-8"> <div class="title">Adversarial Examples Make Strong Poisons</div> <div class="author"> Liam Fowl ,  <em>Micah Goldblum</em> ,  Ping-yeh Chiang ,  Jonas Geiping ,  Wojtek Czaja ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2106.10807" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The adversarial machine learning literature is largely partitioned into evasion attacks on testing data and poisoning attacks on training data. In this work, we show that adversarial examples, originally intended for attacking pre-trained models, are even more effective for data poisoning than recent methods designed specifically for poisoning. Our findings indicate that adversarial examples, when assigned the original label of their natural base image, cannot be used to train a classifier for natural images. Furthermore, when adversarial examples are assigned their adversarial class label, they are useful for training. This suggests that adversarial examples contain useful semantic content, just with the “wrong” labels (according to a network, but not a human). Our method, adversarial poisoning, is substantially more effective than existing poisoning methods for secure dataset release, and we release a poisoned version of ImageNet, ImageNet-P, to encourage research into the strength of this form of data obfuscation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cutmix-480.webp 480w,/assets/img/publication_preview/cutmix-800.webp 800w,/assets/img/publication_preview/cutmix-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/cutmix.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cutmix.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ni2021data" class="col-sm-8"> <div class="title">Data Augmentation for Meta-Learning</div> <div class="author"> Renkun Ni ,  <em>Micah Goldblum</em> ,  Amr Sharaf ,  Kezhi Kong ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2010.07092" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Conventional image classifiers are trained by randomly sampling mini-batches of images. To achieve state-of-the-art performance, practitioners use sophisticated data augmentation schemes to expand the amount of training data available for sampling. In contrast, meta-learning algorithms sample support data, query data, and tasks on each training step. In this complex sampling scenario, data augmentation can be used not only to expand the number of images available per class, but also to generate entirely new classes/tasks. We systematically dissect the meta-learning pipeline and investigate the distinct ways in which data augmentation can be integrated at both the image and class levels. Our proposed meta-specific data augmentation significantly improves the performance of meta-learners on few-shot classification benchmarks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/data-poisoning-480.webp 480w,/assets/img/publication_preview/data-poisoning-800.webp 800w,/assets/img/publication_preview/data-poisoning-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/data-poisoning.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="data-poisoning.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schwarzschild2020just" class="col-sm-8"> <div class="title">Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks</div> <div class="author"> Avi Schwarzschild ,  <em>Micah Goldblum</em> ,  Arjun Gupta ,  John P Dickerson ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2006.12557" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, it remains unclear exactly how dangerous poisoning methods are and which ones are more effective considering that these methods, even ones with identical objectives, have not been tested in consistent or realistic settings. We observe that data poisoning and backdoor attacks are highly sensitive to variations in the testing setup. Moreover, we find that existing methods may not generalize to realistic settings. While these existing works serve as valuable prototypes for data poisoning, we apply rigorous tests to determine the extent to which we should fear them. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lowkey-480.webp 480w,/assets/img/publication_preview/lowkey-800.webp 800w,/assets/img/publication_preview/lowkey-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/lowkey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lowkey.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cherepanova2021lowkey" class="col-sm-8"> <div class="title">LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition</div> <div class="author"> Valeriia Cherepanova ,  <em>Micah Goldblum</em> ,  Harrison Foley ,  Shiyuan Duan ,  John P Dickerson ,  Gavin Taylor ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2101.07922" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Facial recognition systems are increasingly deployed by private corporations, government agencies, and contractors for consumer services and mass surveillance programs alike. These systems are typically built by scraping social media profiles for user images. Adversarial perturbations have been proposed for bypassing facial recognition systems. However, existing methods fail on full-scale systems and commercial APIs. We develop our own adversarial filter that accounts for the entire image processing pipeline and is demonstrably effective against industrial-grade pipelines that include face detection and large scale databases. Additionally, we release an easy-to-use webtool that significantly degrades the accuracy of Amazon Rekognition and the Microsoft Azure Face Recognition API, reducing the accuracy of each to below 1%.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/manifold-480.webp 480w,/assets/img/publication_preview/manifold-800.webp 800w,/assets/img/publication_preview/manifold-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/manifold.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="manifold.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pope2021intrinsic" class="col-sm-8"> <div class="title">The Intrinsic Dimension of Images and Its Impact on Learning</div> <div class="author"> Phillip Pope ,  Chen Zhu ,  Ahmed Abdelkader ,  <em>Micah Goldblum</em> ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2104.08894" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations. This idea underlies a common intuition for the remarkable success of deep learning in computer vision. In this work, we apply dimension estimation tools to popular datasets and investigate the role of low-dimensional structure in deep learning. We find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images. Additionally, we find that low dimensional datasets are easier for neural networks to learn, and models solving these tasks generalize better from training to test data. Along the way, we develop a technique for validating our dimension estimation tools on synthetic data generated by GANs allowing us to actively manipulate the intrinsic dimension by controlling the image generation process.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/instahide-480.webp 480w,/assets/img/publication_preview/instahide-800.webp 800w,/assets/img/publication_preview/instahide-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/instahide.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="instahide.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="borgnia2021strong" class="col-sm-8"> <div class="title">Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff</div> <div class="author"> Eitan Borgnia ,  Valeriia Cherepanova ,  Liam Fowl ,  Amin Ghiasi ,  Jonas Geiping ,  <em>Micah Goldblum</em> ,  Tom Goldstein ,  and  Arjun Gupta </div> <div class="periodical"> <em>In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2011.09527" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Data poisoning and backdoor attacks manipulate victim models by maliciously modifying training data. In light of this growing threat, a recent survey of industry professionals revealed heightened fear in the private sector regarding data poisoning. Many previous defenses against poisoning either fail in the face of increasingly strong attacks, or they significantly degrade performance. However, we find that strong data augmentations, such as mixup and CutMix, can significantly diminish the threat of poisoning and backdoor attacks without trading off performance. We further verify the effectiveness of this simple defense against adaptive poisoning methods, and we compare to baselines including the popular differentially private SGD (DP-SGD) defense. In the context of backdoors, CutMix greatly mitigates the attack while simultaneously increasing validation accuracy by 9%.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/hft-480.webp 480w,/assets/img/publication_preview/hft-800.webp 800w,/assets/img/publication_preview/hft-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/hft.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hft.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="goldblum2021adversarial" class="col-sm-8"> <div class="title">Adversarial Attacks on Machine Learning Systems for High-Frequency Trading</div> <div class="author"> <em>Micah Goldblum</em> ,  Avi Schwarzschild ,  Ankit B Patel ,  and  Tom Goldstein </div> <div class="periodical"> <em>ACM International Conference on AI in Finance (ICAIF)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2002.09565" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Algorithmic trading systems are often completely automated, and deep learning is increasingly receiving attention in this domain. Nonetheless, little is known about the robustness properties of these models. We study valuation models for algorithmic trading from the perspective of adversarial machine learning. We introduce new attacks specific to this domain with size constraints that minimize attack costs. We further discuss how these attacks can be used as an analysis tool to study and evaluate the robustness properties of financial models. Finally, we investigate the feasibility of realistic adversarial attacks in which an adversarial trader fools automated trading systems into making inaccurate predictions.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/meta-480.webp 480w,/assets/img/publication_preview/meta-800.webp 800w,/assets/img/publication_preview/meta-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/meta.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="meta.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="goldblum2020adversarially1" class="col-sm-8"> <div class="title">Adversarially Robust Few-Shot Learning: A Meta-Learning Approach</div> <div class="author"> <em>Micah Goldblum</em> ,  Liam Fowl ,  and  Tom Goldstein </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/1910.00982" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Previous work on adversarially robust neural networks for image classification requires large training sets and computationally expensive training procedures. On the other hand, few-shot learning methods are highly vulnerable to adversarial examples. The goal of our work is to produce networks which both perform well at few-shot classification tasks and are simultaneously robust to adversarial examples. We develop an algorithm, called Adversarial Querying (AQ), for producing adversarially robust meta-learners, and we thoroughly investigate the causes for adversarial vulnerability. Moreover, our method achieves far superior robust performance on few-shot image classification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/variance_2-480.webp 480w,/assets/img/publication_preview/variance_2-800.webp 800w,/assets/img/publication_preview/variance_2-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/variance_2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="variance_2.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="goldblum2020unraveling" class="col-sm-8"> <div class="title">Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks</div> <div class="author"> <em>Micah Goldblum</em> ,  Steven Reich ,  Liam Fowl ,  Renkun Ni ,  Valeriia Cherepanova ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2002.06753" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Meta-learning algorithms produce feature extractors which achieve state-of-the-art performance on few-shot classification. While the literature is rich with meta-learning methods, little is known about why the resulting feature extractors perform so well. We develop a better understanding of the underlying mechanics of meta-learning and the difference between models trained using meta-learning and models which are trained classically. In doing so, we introduce and verify several hypotheses for why meta-learned models perform better. Furthermore, we develop a regularizer which boosts the performance of standard training routines for few-shot classification. In many cases, our routine outperforms meta-learning while simultaneously running an order of magnitude faster.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/propaganda-480.webp 480w,/assets/img/publication_preview/propaganda-800.webp 800w,/assets/img/publication_preview/propaganda-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/propaganda.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="propaganda.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="goldblum2019truth" class="col-sm-8"> <div class="title">Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory</div> <div class="author"> <em>Micah Goldblum</em> ,  Jonas Geiping ,  Avi Schwarzschild ,  Michael Moeller ,  and  Tom Goldstein </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/1910.00359" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/distilled-480.webp 480w,/assets/img/publication_preview/distilled-800.webp 800w,/assets/img/publication_preview/distilled-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/distilled.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="distilled.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="goldblum2020adversarially2" class="col-sm-8"> <div class="title">Adversarially Robust Distillation</div> <div class="author"> <em>Micah Goldblum</em> ,  Liam Fowl ,  Soheil Feizi ,  and  Tom Goldstein </div> <div class="periodical"> <em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/1905.09747" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Knowledge distillation is effective for producing small, high-performance neural networks for classification, but these small networks are vulnerable to adversarial attacks. This paper studies how adversarial robustness transfers from teacher to student during knowledge distillation. We find that a large amount of robustness may be inherited by the student even when distilled on only clean images. Second, we introduce Adversarially Robust Distillation (ARD) for distilling robustness onto student networks. In addition to producing small models with high test accuracy like conventional distillation, ARD also passes the superior robustness of large networks onto the student. In our experiments, we find that ARD student models decisively outperform adversarially trained networks of identical architecture in terms of robust accuracy, surpassing state-of-the-art methods on standard robustness benchmarks. Finally, we adapt recent fast adversarial training methods to ARD for accelerated robust distillation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/witch-480.webp 480w,/assets/img/publication_preview/witch-800.webp 800w,/assets/img/publication_preview/witch-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/witch.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="witch.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chiang2020witchcraft" class="col-sm-8"> <div class="title">Witchcraft: Efficient PGD Attacks with Random Step Size</div> <div class="author"> Ping-Yeh Chiang ,  Jonas Geiping ,  <em>Micah Goldblum</em> ,  Tom Goldstein ,  Renkun Ni ,  Steven Reich ,  and  Ali Shafahi </div> <div class="periodical"> <em>In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/1911.07989" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>State-of-the-art adversarial attacks on neural networks use expensive iterative methods and numerous random restarts from different initial points. Iterative FGSM-based methods without restarts trade off performance for computational efficiency because they do not adequately explore the image space and are highly sensitive to the choice of step size. We propose a variant of Projected Gradient Descent (PGD) that uses a random step size to improve performance without resorting to expensive random restarts. Our method, Wide Iterative Stochastic crafting (WITCHcraft), achieves results superior to the classical PGD attack on the CIFAR-10 and MNIST data sets but without additional computational cost. This simple modification of PGD makes crafting attacks more economical, which is important in situations like adversarial training where attacks need to be crafted in real time.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/shearlet-480.webp 480w,/assets/img/publication_preview/shearlet-800.webp 800w,/assets/img/publication_preview/shearlet-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/shearlet.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="shearlet.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="goldblum2019sheared" class="col-sm-8"> <div class="title">Sheared Multi-Scale Weight Sharing for Multi-Spectral Superresolution</div> <div class="author"> <em>Micah Goldblum</em> ,  Liam Fowl ,  and  Wojciech Czaja </div> <div class="periodical"> <em>In Algorithms, Technologies, and Applications for Multispectral and Hyperspectral Imagery XXV</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10986/109860X/Sheared-multi-scale-weight-sharing-for-multi-spectral-superresolution/10.1117/12.2519982.short?SSO=1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Deep learning approaches to single-image superresolution typically use convolutional neural networks. Convolutional layers introduce translation invariance to neural networks. However, other spatial invariants appear in imaging data. Two such invariances are scale invariance, similar features at multiple spacial scales, and shearing invariance. We investigate these invariances by using weight sharing between dilated and sheared convolutional kernels in the context of multi-spectral imaging data. Traditional pooling methods can extract features at coarse spacial levels. Our approach explores a finer range of scales. Additionally, our approach offers improved storage efficiency because dilated and sheared convolutions allows single trainable kernels to extract information at multiple spacial scales and shears without the costs of training and storing many filters, especially in multi-spectral imaging where data representations are complex.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Micah Goldblum. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>